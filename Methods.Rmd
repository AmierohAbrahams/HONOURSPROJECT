---
title: "Methods"
author: "Amieroh Abrahams"
date: "7 May 2018"
output:
  word_document:
    toc: yes
  html_document:
    fig_caption: yes
    fig_height: 5
    fig_width: 5
    highlight: default
    number_sections: yes
    theme: paper
    toc: yes
    toc_float: yes
  pdf_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    highlight: zenburn
    latex_engine: xelatex
mainfont: PT Serif
monofont: PT Mono
language: Australian
sansfont: PT Sans
fontsize: 12pt
---

## Methods

First I need to find, install and load various packages. These packages will be available on CRAN and can be accessed and installed in the usual way.

```{r prelim_opts, echo=FALSE}
knitr::opts_chunk$set(
  comment = "R>", 
  warning = FALSE, 
  message = FALSE
)

library(tidyverse)
library(ggpubr)
library(zoo)
library(lubridate)
library(ggrepel)
library(FNN)
library(stringr)
```

Now to get to the data. The first step involves the loading of the site list. 

```{r load_files1, include=FALSE}
load("site_list_v4.2.RData")
```

Performing the k-means clustering on a data matrix using the `kmeans()` function, which uses multiple random seeds to find a number of clusting solutions; it selects as the final solution the one that has the minimum total within-cluster sum of squared distances. K-means attempts to minimise distortion. At the minimum, all cluster centres are at the mean of the set of data points which are nearest to the cluster centre. The data contain variables from many sites along the South African coast, but in the analysis I use only some of them. These variables are the mean, min and max. 

```{r setup_k-means, include=FALSE}
set.seed(10)
kmeans(site_list[,c(15, 18, 19)], 3)$cluster
clust_columns <- site_list[,c(15, 18:19)]
```

Visualisation of the clusters. My plotting functions partition the data into the clusters and colour code each point accordingly so that I am able to see patterns that exist.

```{r define_fun1, include=FALSE}
clust_i <- function(i) {
  set.seed(10)
  ggplot(data = site_list, 
         aes(x = lon, y = lat, 
             colour = as.factor(kmeans(clust_columns, i)$cluster))) +
    borders() +
    geom_point() +
    labs(colour = "cluster") +
    coord_equal(xlim = c(15, 35), ylim = c(-37, -27)) +
    ggtitle(paste0("clust = ", i))
}
```

```{r run_fun1, include=FALSE}
clust_6 <- clust_i(6)
clust_5 <- clust_i(5)
clust_4 <- clust_i(4)
clust_3 <- clust_i(3)

clusters <- ggarrange(clust_6, clust_5, clust_4, clust_3, common.legend = T)
clusters
```

It is now necessary to load the SACTN dataset. The SACTN dataset comprise of 129 *in situ* coastal seawater temperatures derived from daily measurements over up to 40 years. Clustering the sites using the k-means results:

```{r load_files2, include=FALSE}
load("SACTN_data/SACTN_daily_v4.2.RData")
```

Creating a cluster index whilst only selecting cluster 4. Working with one decade of data for each of the sites along the coast.

```{r run_k-means, include=FALSE}
set.seed(10)
site_list$cluster <- as.factor(kmeans(site_list[,c(15, 18:19)], 4)$cluster)

site_list_clusters <- site_list %>% 
  filter(length >= 3650)

SACTN_daily_clusters <- left_join(SACTN_daily_v4.2, site_list[,c(4, 13, 21)]) %>% 
  filter(length >= 3650) 
```

With the sites now split into four clusters, the next step is to reduce the number of sites per cluster down to a manageable, but still representative sub-sample of the whole. THis is done primarily for two reasons. The first, simply, is to allow for the comparisons to be more readily interpretable by humans. The second more objective reason is to allow for an equal amount of sampling per cluster. The East coast is much more heavily sampled than the rest and so this imbalance must be addressed. The criteria that must be considered for the sub-samples are that both instrument types (UTR and thermometre) are present, and as many sources are included as are possible.

```{r site_sub_samples, include=FALSE}
# First simply divide up the site list by cluster
sites_1 <- site_list_clusters %>% 
  filter(cluster == 1)
sites_2 <- site_list_clusters %>% 
  filter(cluster == 2)
sites_3 <- site_list_clusters %>% 
  filter(cluster == 3)
sites_4 <- site_list_clusters %>% 
  filter(cluster == 4)

# I then grouped the sub-samples by source and took the longest time series
# When fewer than 4 sources were present, I took the next longest time series etc.
sites_1_sub <- sites_1 %>% 
  group_by(src) %>%
  filter(length %in% head(length, 2)) %>% 
  ungroup()
sites_2_sub <- sites_2 %>% 
  group_by(src) %>%
  filter(length %in% head(length, 3)) %>% 
  ungroup()
sites_3_sub <- sites_3 %>% 
  group_by(src) %>%
  filter(length %in% head(length, 2)) %>% 
  ungroup() %>% 
  slice(1:4) 
# These sites don't all overlap...
sites_4_sub <- sites_4 %>% 
  filter(src != "SAWS") %>% # The SAWS data time series doesn't match up
  group_by(src) %>%
  filter(length %in% head(length, 2)) %>% 
  filter(NA.perc %in% tail(NA.perc, 2)) %>% 
  ungroup() %>% 
  slice(c(1,4,7,12))

# 
# This then creates subsets of the four sites per cluster
# Though we may want to consider the distance between sites as well...
# Anyway, for now I stitch them together to be used as a further index 
# for subsetting the daily data
site_list_sub <- rbind(sites_1_sub, sites_2_sub, sites_3_sub, sites_4_sub)

# Then subset the daily data
SACTN_daily_clusters_sub <- SACTN_daily_clusters %>% 
  filter(index %in% site_list_sub$index)
```

The function droplevels is used here to drop unused levels from the factor such as the length column. The unique function is then used to determine the amount sites found within this cluster:

```{r, include=FALSE}
# SACTN_clust_1 <- SACTN_daily_clusters %>% 
#   filter(cluster == 1) %>% 
#   select(-length) %>% 
#   droplevels()
# length(unique(SACTN_clust_1$index))
```

Creating a function matching the sites:

```{r define_fun2, include=FALSE}
## testing...
# df <- SACTN_daily_clusters
# clust <- 1
##
clust_match <- function(df, clust) {
  SACTN_clust_x_match <- data.frame()
  df2 <- df %>% 
    filter(cluster == clust) %>%
    select(-length) %>%
    droplevels()
  for (i in 1:length(levels(df2$index))) {
    # for (i in 1:10) {
    SACTN_df_1 <- filter(df2, index == levels(index)[i])
    for (j in 1:length(levels(df2$index))) {
      # for(j in 1:10) {
      if (i == j) {
        next
      }
      if (j < i) {
        next
      }
      SACTN_df_2 <- filter(df2, index == levels(index)[j])
      SACTN_df_3 <- left_join(SACTN_df_1, SACTN_df_2, by = "date") %>%
        na.trim() %>%
        select(date, index.x, temp.x, index.y, temp.y, -cluster.x, -cluster.y) %>%
        rename(index_1 = index.x,
          temp_1 = temp.x,
          index_2 = index.y,
          temp_2 = temp.y) %>%
        mutate(index_pair = paste0(index_1, " - ", index_2))
      SACTN_clust_x_match <- rbind(SACTN_clust_x_match, SACTN_df_3)
    }
  }
  return(SACTN_clust_x_match)
}
```

I calculated the monthly mean temperature and standard deviation for each year and site found in cluster 1. Consequently we have a data frame with 5 columns and a row for each of months per year. 

```{r run_fun2, include=FALSE}
# Calculate all of the matche-ups in cluster 1
SACTN_clust_1_match <- clust_match(df = SACTN_daily_clusters_sub, clust = 1)
SACTN_clust_2_match <- clust_match(df = SACTN_daily_clusters_sub, clust = 2)
SACTN_clust_3_match <- clust_match(df = SACTN_daily_clusters_sub, clust = 3)
SACTN_clust_4_match <- clust_match(df = SACTN_daily_clusters_sub, clust = 4)


clust_legit <- function(df) {
  SACTN_clust_x_legit <- df %>% 
  mutate(temp_legit = temp_1-temp_2,
         month = month(date, abbr = T, label = T),
         year = year(date)) %>% 
  select(-temp_1, -temp_2) %>% 
  group_by(index_pair, month, year) %>% 
  summarise(temp_mean_month_year = mean(temp_legit, na.rm = T),
            temp_sd_month_year = sd(temp_legit, na.rm = T))
  return(SACTN_clust_x_legit)
}

SACTN_clust_1_legit <- clust_legit(df = SACTN_clust_1_match)
SACTN_clust_2_legit <- clust_legit(df = SACTN_clust_2_match)
SACTN_clust_3_legit <- clust_legit(df = SACTN_clust_3_match)
SACTN_clust_4_legit <- clust_legit(df = SACTN_clust_4_match)
```

The code below lets us visualise the temperature variation at each of the sites found in cluster 1

```{r define_fun3, include=FALSE}
clust_plot <- function(df) {
  plot1 <- ggplot(df, aes(x = index_pair)) +
    geom_boxplot(aes(y = temp_1), fill = "khaki4", alpha = 0.3) +
    geom_boxplot(aes(y = temp_2), fill = "chartreuse3", alpha = 0.3) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  return(plot1)
}
```

```{r run_fun3,  fig.cap = "Site locations along the South African coast represented by various temperature parameters. Temperature parameters include minimum, maximum and mean temperatures (Â° Celsius).", fig.height = 8, fig.width = 12}
SACTN_clust_1_plot1 <- clust_plot(df = SACTN_clust_1_match)
SACTN_clust_1_plot1
SACTN_clust_2_plot1 <- clust_plot(df = SACTN_clust_2_match)
SACTN_clust_2_plot1
SACTN_clust_3_plot1 <- clust_plot(df = SACTN_clust_3_match)
SACTN_clust_3_plot1
SACTN_clust_4_plot1 <- clust_plot(df = SACTN_clust_4_match)
SACTN_clust_4_plot1
```
The general trend is that ocean temperature varies across the various sites. On average, Sodwana has the highest temperature and Port Nolloth has the lowest. In general however, most sites have relatively similar average temperatures with several sites having near identical temperatures (for example, Gansbaai and Cape Aghulus which has nearly no difference between them), likely owing to their geographical locations. Furthermore, we see that the temperatures of sites occurring along the same coasts have similar temperatures, but temperatures differ between coasts as we would expect.


Now I make a visualisation to reveal the relationship between the mean temperature for each month of each year for each of the sites in cluster:

```{r define_fun4, include=FALSE}
clust_plot2 <- function(df){
  plot2 <- df %>% 
    ggplot(aes(x = year, y = temp_mean_month_year)) +
    geom_line(aes(group = index_pair, colour = index_pair), alpha = 0.7, show.legend = F) +
    geom_smooth(method = "gam", se = F, aes(colour = index_pair), show.legend = T)  +
    facet_wrap(~month) + 
    theme_pubclean()
  return(plot2)
}
```

```{r run_fun, fig.cap = "Site locations represented by the monthly average temperatures between two sites (Â° Celsius). Each graphic shows a different month overtime from 1990 to 2016. These site locations are coloured by the temperature statistic relative to the legends provided.", fig.height = 8, fig.width = 12}
SACTN_clust_1_plot2 <- clust_plot2(df = SACTN_clust_1_legit)
SACTN_clust_1_plot2
SACTN_clust_2_plot2 <- clust_plot2(df = SACTN_clust_2_legit)
SACTN_clust_2_plot2
SACTN_clust_3_plot2 <- clust_plot2(df = SACTN_clust_3_legit)
SACTN_clust_3_plot2
SACTN_clust_4_plot2 <- clust_plot2(df = SACTN_clust_4_legit)
SACTN_clust_4_plot2
```
Over a period of 16 years, we found that changes in average ocean temperatures did not occur uniformly across our various sites along the coasts of South Africa. Some sites increased in temperature at faster rates than others, for example Gansbaai increased in temperature faster than Cape Aghulus over the same time period. In addition, we found instances of seasonal variation having an effect on the changes to average ocean temperatures at a yearly rate, and again this does not occur uniformly across coastal sites.


Now I make a visualisation to reveal the relationship between the sd temperature for each month of each year for each of the sites in cluster

```{r define_fun5, include=False}
clust_plot3 <- function(df){
  plot3 <- df %>% 
    ggplot(aes(x = year, y = temp_sd_month_year)) +
  geom_line(aes(group = index_pair, colour = index_pair), alpha = 0.7) +
  geom_smooth(method = "gam", se = F, aes(colour = index_pair))  +
  facet_wrap(~month) + 
  theme_pubclean()
  return(plot3)
}
```

```{r run_fun, fig.cap = "fig.cap = "Site locations represented by temperature parameters. Temperature parameters include monthly standard deviation between two sites (Â° Celsius). Each graphic shows a different month overtime from 1990 to 2016. These site locations are coloured by the temperature statistic relative to the legends provided.", fig.height = 8, fig.width = 12}
SACTN_clust_1_plot3 <- clust_plot3(df = SACTN_clust_1_legit)
SACTN_clust_1_plot3
SACTN_clust_2_plot3 <- clust_plot3(df = SACTN_clust_2_legit)
SACTN_clust_2_plot3
SACTN_clust_3_plot3 <- clust_plot3(df = SACTN_clust_3_legit)
SACTN_clust_3_plot3
SACTN_clust_4_plot3 <- clust_plot3(df = SACTN_clust_4_legit)
SACTN_clust_4_plot3
```


Creating a function which allows me to load all the wave data:

```{r define_fun4}
test_func <- function(wave_files) {
  site_type <- sapply(strsplit(as.character(wave_files), "/"), "[[", 3)
  site_num <- sapply(strsplit(as.character(wave_files), "/"), "[[", 4)
  site_num <- sapply(strsplit(site_num, ".txt"), "[[", 1)
  site_name <- paste(site_type, site_num, sep = "")

  try1 <- read.csv(wave_files, col.names = c("date", "hs", "tp", "dir", "dirw", "spw"), sep = "", header = F) %>%
    filter(tp != -999) %>% 
    mutate(date = as.POSIXct(as.character(date), "%Y%m%d%H%M", tz = "Africa/Johannesburg")) %>%
    mutate(site = site_name) %>%
    select(site, everything())
  return(try1)
}
```

Feeding as a vector:

```{r, include=FALSE}
trial_load <- function(directory) {
  files_list <- dir(directory, full.names = TRUE)
  final <- plyr::ldply(files_list, test_func, .progress = "text")
  return(final)
}
```

Loading the wave data for the sites along the South African coast:

```{r, include=FALSE}
sites_complete <- read_csv("data/sites_complete.csv") %>% 
  select(-site_list) %>% 
  rename(site_real = site) %>% 
  gather(key = "depth", value = "site", -site_real, -lon, -lat)

FB <- trial_load("data/wave_data/FB")
TB <- trial_load("data/wave_data/TB")
HE <- trial_load("data/wave_data/HE")
SH <- trial_load("data/wave_data/SH")

wave_data <- rbind(FB, TB, HE, SH)
wave_data <- left_join(wave_data, sites_complete) %>% 
  rename(site_wave = site,
         site = site_real)
rm(FB, TB, HE, SH)
```

Converting the 3-hour resolution wave data into daily data. The funs function, funs(), is used here as it provides a flexible way to generate a named list of functions for input to other functions such as the mean and sd:

```{r, include=FALSE}
wave_daily <- wave_data %>% 
  mutate(date = as.Date(date)) %>% 
  group_by(site, site_wave, depth, lon, lat, date) %>% 
  summarise_all(funs(mean = mean, sd = sd), na.rm = T) %>% 
  ungroup()
```

Calculating the annual wave data for each of the sites excluding the year 2000:

```{r, include=FALSE}
wave_annually <- wave_data %>% 
  mutate(date = lubridate::year(date)) %>% 
  group_by(site, site_wave, depth, lon, lat, date) %>% 
  summarise_all(funs(mean = mean, sd = sd), na.rm = TRUE) %>%
  ungroup() %>%
  filter(date != 2000)
```

Now splitting the wave data into the different depths; either a depth of 7m or a depth of 15m:

```{r unique_name1}
sites_complete_7 <- sites_complete %>% 
  filter(depth == "wave_7")

sites_complete_15 <- sites_complete %>% 
  filter(depth == "wave_15")
```

Creating a time series over a period of 10 years:

```{r}
site_list_10_years <- site_list %>% 
  filter(index %in% SACTN_daily_clusters$index)
```

Using the FNN package to determine the nearest wave data for each of the SACTN sites present in this cluster. Once that was determined only waves minimised to only include the 7 and 15m depths. Thereafter wave 7 and wave 15 was combined and all unnessasary infomationwas removed and the cluster column was then added to the sites infomation:

```{r unique_name2}
waves_idx_7 <- as.data.frame(knnx.index( as.matrix(sites_complete_7[,2:3]), as.matrix(site_list_10_years[,5:6]), k = 1)) 
waves_idx_15 <- as.data.frame(knnx.index( as.matrix(sites_complete_15[,2:3]), as.matrix(site_list_10_years[,5:6]), k = 1)) 

sites_complete_subset_7 <- sites_complete_7[as.matrix(waves_idx_7),]
sites_complete_subset_15 <- sites_complete_15[as.matrix(waves_idx_15),]
```

```{r}
sites_complete_subset <- cbind(sites_complete_subset_7, sites_complete_subset_15[,4:5])
colnames(sites_complete_subset)[c(5, 7)] <- c("wave_7", "wave_15")
sites_complete_subset <- sites_complete_subset[, c(1:3, 5, 7)]

site_list_match <- cbind(site_list_10_years[,c(4:6,21)], sites_complete_subset)
colnames(site_list_match) <- c("index", "lon1", "lat1", "cluster", "site_real", "lon2", "lat2", "wave_7", "wave_15")
```

Creating a function using the haversine formula to calculate the geodesic distance between two points specified by radian lat/lon, which is the shortest distance over the earthâs surface. This formula is particularly well-conditioned for numerical computations at both large and small scales:

```{r define_fun5}
deg2rad <- function(deg) return(deg*pi/180)
gcd.hf <- function(long1, lat1, long2, lat2) {
  R <- 6371 
  delta.long <- (long2 - long1)
  delta.lat <- (lat2 - lat1)
  a <- sin(delta.lat/2)^2 + cos(lat1) * cos(lat2) * sin(delta.long/2)^2
  c <- 2 * asin(min(1,sqrt(a)))
  d <- R * c
  return(d) 
}
```

Creating a function to calculate matrix of distances between each two sites following the haversine formula:

```{r define_fun6}
CalcDists <- function(latlongs) {
  name <- list(rownames(latlongs), rownames(latlongs))
  n <- nrow(latlongs)
  z <- matrix(0, n, n, dimnames = name)
  for (i in 1:n) {
    for (j in 1:n) z[i, j] <- gcd.hf(long1 = latlongs[i, 1],
                                     lat1 = latlongs[i, 2], long2 = latlongs[j, 1], lat2 = latlongs[j,2])
  }
  z <- as.dist(z)
  return(z)
}
```

```{r}
PairsDists <- function(latlongs) {
  n <- nrow(latlongs)
  z <- matrix(0, n, 1, dimnames = list(latlongs[,1]))
  for (i in 1:n) {
    z[i] <- gcd.hf(long1 = latlongs[i, 2], lat1 = latlongs[i, 3],
                   long2 = latlongs[i+1, 2], lat2 = latlongs[i+1,3])
  }
  return(z)
}
```

Once the sites were matched with the sites of the 10 year time series, the distance was converted to radians. Hereafter the wave data were combined with the temperature data at a depth of 7 and 15m respectivley. 

```{r unique_name3}
site_list_match_dist <- site_list_match %>%
  mutate(lon1 = deg2rad(lon1),
         lat1 = deg2rad(lat1),
         lon2 = deg2rad(lon2),
         lat2 = deg2rad(lat2),
         dist = NA)

# Creating a loop for dstances
for(i in 1:nrow(site_list_match_dist)) {
  dist <- gcd.hf(site_list_match_dist$lon1[i], site_list_match_dist$lat1[i], 
                 site_list_match_dist$lon2[i], site_list_match_dist$lat2[i])
  site_list_match_dist$dist[i] <- dist
}

site_list_10_years <- left_join(site_list_10_years, site_list_match_dist[, c(1, 8:10)])

# Up next we want to combine the temperature and wave data
# First for the 7 metre depth waves
SACTN_daily_clusters_7 <- left_join(SACTN_daily_clusters, site_list_10_years[, c(4, 22)]) %>% 
  left_join(wave_daily[, c(2:3, 6:16)], by = c("date", "wave_7" = "site_wave"))

# Then the 15 metre depth waves
SACTN_daily_clusters_15 <- left_join(SACTN_daily_clusters, site_list_10_years[, c(4, 23)]) %>% 
  left_join(wave_daily, by = c("date", "wave_15" = "site_wave"))

SACTN_daily_clusters_7 %>%
  group_by(index) %>%
  summarise(temp_dir_cor = cor(temp, dir_mean))
```


```{r plt2}
ggplot(data = SACTN_daily_clusters_7, aes(x = temp, y = dir_mean)) +
  geom_line() +
  facet_wrap(~index)
```










